{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tvt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, SVHN, Places365\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "from pytorch_ood.dataset.img import (\n",
    "    LSUNCrop,\n",
    "    Textures,\n",
    "    TinyImageNetCrop,\n",
    "    GaussianNoise\n",
    ")\n",
    "from pytorch_ood.detector import (\n",
    "    ODIN,\n",
    "    EnergyBased,\n",
    "    Entropy,\n",
    "    KLMatching,\n",
    "    Mahalanobis,\n",
    "    MaxLogit,\n",
    "    MaxSoftmax,\n",
    "    ViM,\n",
    ")\n",
    "from pytorch_ood.model import WideResNet\n",
    "from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "trans = tvt.Compose([\n",
    "    tvt.Resize(size=(32, 32)),\n",
    "    ToRGB(),\n",
    "    tvt.ToTensor(),\n",
    "    tvt.Normalize(std=std, mean=mean)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(test_loader, detector):\n",
    "    metrics = OODMetrics() # Evaluate Detectors\n",
    "    \n",
    "    print(\"Total count:\", len(test_loader))\n",
    "    cnt = 0\n",
    "    for x, y in test_loader:\n",
    "        metrics.update(detector(x.cuda()), y)\n",
    "        cnt += 1\n",
    "        if (cnt + 1) % 50 == 0:\n",
    "            print(f\"[{cnt}/{len(test_loader)}]\")\n",
    "    \n",
    "    r = metrics.compute()\n",
    "    print(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checks.\n",
    "# dataset_out_test = Places365(root=\"data\", split=\"val\", small=True, transform=trans, target_transform=ToUnknown(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_all_datasets(detector, base_dataset, transform=trans, batch_size=128, num_workers=20):\n",
    "    dataset_names = [\"SVHN\", \"Textures\", \"LSUNCrop\", \"TinyImageNetCrop\", \"Places365\", \"GaussianNoise\"]\n",
    "    results = []\n",
    "\n",
    "    for name in dataset_names:\n",
    "        if name == \"SVHN\":\n",
    "            dataset_out_test = SVHN(root=\"data\", split=\"test\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"Textures\":\n",
    "            dataset_out_test = Textures(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"LSUNCrop\":\n",
    "            dataset_out_test = LSUNCrop(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"TinyImageNetCrop\":\n",
    "            dataset_out_test = TinyImageNetCrop(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"Places365\":\n",
    "            dataset_out_test = Places365(root=\"data\", split=\"val\", small=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"GaussianNoise\":\n",
    "            dataset_out_test = GaussianNoise(length=10000, transform=transform, target_transform=ToUnknown())\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        test_loader = DataLoader(torch.utils.data.ConcatDataset([base_dataset, dataset_out_test]), batch_size=batch_size, num_workers=num_workers)\n",
    "        print()\n",
    "        print(name)\n",
    "        results.append(calculate(test_loader, detector))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zZ6VGBU33uhf"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OODDector(nn.Module):\n",
    "    def __init__(self, num_classes=10, multi_class=True):\n",
    "        super().__init__()\n",
    "        self.fc_out = nn.Linear(8192, 2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        if multi_class:\n",
    "            k = num_classes + 1\n",
    "        else:\n",
    "            k = 1\n",
    "        self.out_layer = nn.Linear(2048, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc_out(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hh8wXZG8xI1h"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Any, Tuple\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class CIFAR10withOOD(dset.CIFAR10):\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    def __init__(self, root, batch_size, ood_files=None, download=True, train=True, transform=None,\n",
    "                 ood_transform=None, use_patch_aug=True, multiclass=True):\n",
    "        self.root = root\n",
    "        self.ood_files = ood_files\n",
    "        self.num_classes = 10\n",
    "        self.batch_size = batch_size\n",
    "        self.ood_transform = ood_transform\n",
    "        self.use_patch_aug = use_patch_aug\n",
    "        self.multiclass = multiclass\n",
    "        if multiclass:\n",
    "            self.ood_prob = (self.batch_size / (self.num_classes + 1) * 0.01)\n",
    "        else:\n",
    "            self.ood_prob = 0.5\n",
    "        super().__init__(root, download=download, train=train, transform=transform)\n",
    "\n",
    "    def extract_blocks(self, image, mask):\n",
    "        maximum_score = 8 * 8\n",
    "        patch_size = 8\n",
    "        stride = patch_size // 2\n",
    "        patch_images = []\n",
    "        mask_score = []\n",
    "        for i in range(0, 28, stride):\n",
    "            for j in range(0, 28, stride):\n",
    "                if (j + patch_size) > 32:\n",
    "                    break\n",
    "                patch_image = image[i:i + patch_size, j:j + patch_size]\n",
    "                patch_score = maximum_score - np.sum(mask[i:i + patch_size, j:j + patch_size])\n",
    "                if patch_score == maximum_score:\n",
    "                    if np.random.uniform() >= 0.5:\n",
    "                        resize_image = cv2.resize(patch_image, (32, 32))\n",
    "                    else:\n",
    "                        resize_image = np.tile(patch_image, [4, 4, 1])\n",
    "                    return resize_image\n",
    "                patch_images.append(patch_image)\n",
    "                mask_score.append(patch_score)\n",
    "\n",
    "            if (i + patch_size) > 32:\n",
    "                break\n",
    "\n",
    "        max_idx = np.argmax(mask_score, axis=0)\n",
    "        max_patch = patch_images[max_idx]\n",
    "        if np.random.uniform() >= 0.5:\n",
    "            resize_image = cv2.resize(max_patch, (32, 32))\n",
    "        else:\n",
    "            resize_image = np.tile(max_patch, [4, 4, 1])\n",
    "        return resize_image\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if np.random.uniform() <= self.ood_prob:\n",
    "            rand_idx = np.random.randint(0, len(self.ood_files))\n",
    "            img_file = self.ood_files[rand_idx]\n",
    "            img = np.array(Image.open(img_file).convert('RGB')).astype(np.uint8)\n",
    "            target = self.num_classes\n",
    "\n",
    "            if self.use_patch_aug and np.random.uniform() >= 0.5:\n",
    "                mask_file = img_file.replace(\"_train.png\", \"_mask.png\")\n",
    "                mask = np.array(Image.open(mask_file)) / 255.\n",
    "                mask = np.uint8(mask)\n",
    "                img = self.extract_blocks(img, mask)\n",
    "\n",
    "            img = Image.fromarray(img)\n",
    "            if self.ood_transform is not None:\n",
    "                img = self.ood_transform(img)\n",
    "\n",
    "        else:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5ZpK58P66m6Z"
   },
   "outputs": [],
   "source": [
    "def get_all_blocks(x, model):\n",
    "    out_list = []\n",
    "    out = model.conv1(x)\n",
    "    out = model.block1(out)\n",
    "    out_list.append(out)\n",
    "    out = model.block2(out)\n",
    "    out_list.append(out)\n",
    "    out = model.block3(out)\n",
    "    out = model.relu(model.bn1(out))\n",
    "    out_list.append(out)\n",
    "    return out_list[2].flatten(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import TypeVar\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Self = TypeVar(\"Self\")\n",
    "\n",
    "\n",
    "class RequiresFittingException(Exception):\n",
    "    \"\"\"\n",
    "    Raised when predict is called on a detector that has not been fitted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg=\"You have to call fit() before predict()\"):\n",
    "        super(RequiresFittingException, self).__init__(msg)\n",
    "\n",
    "\n",
    "class ModelNotSetException(ValueError):\n",
    "    \"\"\"\n",
    "    Raised when predict() is called but no model was given.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg=\"When using predict(), model must not be None\"):\n",
    "        super(ModelNotSetException, self).__init__(msg)\n",
    "\n",
    "\n",
    "class Detector(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Base Class for an Out-of-Distribution Detector\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forwards to predict\n",
    "        \"\"\"\n",
    "        return self.predict(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self: Self, data_loader: DataLoader) -> Self:\n",
    "        \"\"\"\n",
    "        Fit the detector to a dataset. Some methods require this.\n",
    "        :param data_loader: dataset to fit on. This is usually the training dataset.\n",
    "        :raise ModelNotSetException: if model was not set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_features(self: Self, x: Tensor, y: Tensor) -> Self:\n",
    "        \"\"\"\n",
    "        Fit the detector directly on features. Some methods require this.\n",
    "        :param x: training features to use for fitting.\n",
    "        :param y: corresponding class labels.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculates outlier scores. Inputs will be passed through the model.\n",
    "        :param x: batch of data\n",
    "        :return: outlier scores for points\n",
    "        :raise RequiresFitException: if detector has to be fitted to some data\n",
    "        :raise ModelNotSetException: if model was not set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_features(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculates outlier scores based on features.\n",
    "        :param x: batch of data\n",
    "        :return: outlier scores for points\n",
    "        :raise RequiresFitException: if detector has to be fitted to some data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OdOnU77ZE9mE"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, TypeVar\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "Self = TypeVar(\"Self\")\n",
    "\n",
    "\n",
    "class Mine(Detector):\n",
    "    \"\"\"\n",
    "    Implements the Maximum Softmax Probability (MSP) Thresholding baseline for OOD detection.\n",
    "    Optionally, implements temperature scaling, which divides the logits by a constant temperature :math:`T`\n",
    "    before calculating the softmax.\n",
    "    .. math:: - \\\\max_y \\\\sigma_y(f(x) / T)\n",
    "    where :math:`\\\\sigma` is the softmax function and :math:`\\\\sigma_y`  indicates the :math:`y^{th}` value of the\n",
    "    resulting probability vector.\n",
    "    :see Paper:\n",
    "        `ArXiv <https://arxiv.org/abs/1610.02136>`_\n",
    "    :see Implementation:\n",
    "        `GitHub <https://github.com/hendrycks/error-detection>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Module, ood_detector: Module, t: Optional[float] = 1.0):\n",
    "        \"\"\"\n",
    "        :param model: neural network to use\n",
    "        :param t: temperature value :math:`T`. Default is 1.\n",
    "        \"\"\"\n",
    "        super(Mine, self).__init__()\n",
    "        self.t = t\n",
    "        self.ood_detector = ood_detector\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param x: input, will be passed through model\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ModelNotSetException\n",
    "\n",
    "        features = get_all_blocks(x, self.model)\n",
    "        pred = self.ood_detector(features.detach())\n",
    "        return pred \n",
    "\n",
    "\n",
    "    def fit(self: Self, *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Not required\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def fit_features(self: Self, *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Not required\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict_features(self, logits: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param logits: logits given by the model\n",
    "        \"\"\"\n",
    "        return MaxSoftmax.score(logits, self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as trn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_ood_detector(ood_dataset_path, model, batch_size=128, epochs=10, seed=42):\n",
    "\n",
    "    #torch.manual_seed(seed)\n",
    "    \n",
    "    ood_transform = [\n",
    "        trn.RandomHorizontalFlip(),\n",
    "        trn.RandomAffine(0, (0.0, 0.0), (1.0, 1.5)),\n",
    "        trn.RandomAutocontrast(0.2),\n",
    "        trn.RandomInvert(0.2),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize(mean, std),\n",
    "    ]\n",
    "    transform_ood = trn.Compose(ood_transform)\n",
    "    \n",
    "    ind_test_data = dset.CIFAR10(f'data', download=True, train=False, transform=trans)\n",
    "    num_classes = 10\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "    \n",
    "    ind_test_loader = DataLoader(ind_test_data, batch_size=batch_size // 2, shuffle=False, num_workers=8)\n",
    "    \n",
    "    args_multi_class = 0\n",
    "    \n",
    "    if args_multi_class:\n",
    "        multi_class = True\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        multi_class = False\n",
    "        criterion = F.binary_cross_entropy_with_logits\n",
    "    \n",
    "    ood_detector = OODDector(num_classes=num_classes, multi_class=multi_class)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(ood_detector.parameters(), lr=0.01, weight_decay=0.0004, momentum=0.8, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        ood_detector.cuda()\n",
    "    \n",
    "    all_files = []\n",
    "    if isinstance(ood_dataset_path, list):  \n",
    "        for path in ood_dataset_path:\n",
    "            all_files.extend(glob.glob(os.path.join(path, \"*.png\")))\n",
    "    else: \n",
    "        all_files.extend(glob.glob(os.path.join(ood_dataset_path, \"*.png\")))\n",
    "    \n",
    "    print(f\"CAM-based OOD NUM : {len(all_files)}\")\n",
    "    \n",
    "    # ood train dataset\n",
    "    concat_dataset = CIFAR10withOOD(f'data', batch_size, ood_files=all_files,\n",
    "                                    download=True, train=True, transform=trans, ood_transform=transform_ood,\n",
    "                                    use_patch_aug=True, multiclass=multi_class)\n",
    "    train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=20, drop_last=True)\n",
    "    \n",
    "    # train ood classifier\n",
    "    for epoch in range(epochs):\n",
    "        loader = tqdm(train_loader, disable=False, ascii=True)\n",
    "        loader.set_description('[%s %03d/%03d]' % ('train', epoch + 1, epochs))\n",
    "        cnt = 0\n",
    "        for batch_idx, (x_data, y_data) in enumerate(loader):\n",
    "            cnt += batch_size\n",
    "    \n",
    "            if torch.cuda.is_available():\n",
    "                x_data = x_data.cuda()\n",
    "                y_data = y_data.cuda()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            features = get_all_blocks(x_data, model)\n",
    "            # print(features.detach())\n",
    "            pred = ood_detector(features.detach())\n",
    "            # print(pred)\n",
    "            if multi_class:\n",
    "                loss = criterion(pred, y_data.view(-1).cuda().to(torch.long))\n",
    "            else:\n",
    "                y_data = torch.where(y_data == num_classes, torch.ones_like(y_data), torch.zeros_like(y_data))\n",
    "                loss = criterion(pred, y_data.view(-1, 1).cuda().to(torch.float32))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        scheduler.step()\n",
    "\n",
    "    return ood_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.27it/s]\n",
      "[train 002/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.87it/s]\n",
      "[train 003/010]: 100%|##############################################| 390/390 [00:05<00:00, 77.97it/s]\n",
      "[train 004/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.63it/s]\n",
      "[train 005/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.22it/s]\n",
      "[train 006/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.49it/s]\n",
      "[train 007/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.73it/s]\n",
      "[train 008/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.64it/s]\n",
      "[train 009/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.00it/s]\n",
      "[train 010/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.48it/s]\n"
     ]
    }
   ],
   "source": [
    "MLM_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_MLM_without_DMR\"\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").cuda().eval()\n",
    "MLM = train_ood_detector(MLM_path, model, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.9671512842178345, 'AUPR-IN': 0.9872857928276062, 'AUPR-OUT': 0.9065738320350647, 'FPR95TPR': 0.17579999566078186}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.9040053486824036, 'AUPR-IN': 0.8908250331878662, 'AUPR-OUT': 0.9053792953491211, 'FPR95TPR': 0.6421999931335449}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9802474975585938, 'AUPR-IN': 0.9724568724632263, 'AUPR-OUT': 0.9830540418624878, 'FPR95TPR': 0.08089999854564667}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9673318862915039, 'AUPR-IN': 0.9621306657791138, 'AUPR-OUT': 0.9692314267158508, 'FPR95TPR': 0.1459999978542328}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.8990402221679688, 'AUPR-IN': 0.9661892652511597, 'AUPR-OUT': 0.7274834513664246, 'FPR95TPR': 0.44909998774528503}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9987630248069763, 'AUPR-IN': 0.9971232414245605, 'AUPR-OUT': 0.9992067217826843, 'FPR95TPR': 0.002899999963119626}\n",
      "AUROC: 0.9527565439542135\n",
      "AURR: 0.9626684784889221\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR10(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, MLM)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|##############################################| 390/390 [00:05<00:00, 66.50it/s]\n",
      "[train 002/010]: 100%|##############################################| 390/390 [00:05<00:00, 68.27it/s]\n",
      "[train 003/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.27it/s]\n",
      "[train 004/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.17it/s]\n",
      "[train 005/010]: 100%|##############################################| 390/390 [00:05<00:00, 77.57it/s]\n",
      "[train 006/010]: 100%|##############################################| 390/390 [00:05<00:00, 77.77it/s]\n",
      "[train 007/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.02it/s]\n",
      "[train 008/010]: 100%|##############################################| 390/390 [00:05<00:00, 76.83it/s]\n",
      "[train 009/010]: 100%|##############################################| 390/390 [00:05<00:00, 77.20it/s]\n",
      "[train 010/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.22it/s]\n"
     ]
    }
   ],
   "source": [
    "DMR_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_single\"\n",
    "#DMR_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_multiple6\"\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").cuda().eval()\n",
    "DMR_single = train_ood_detector(DMR_path, model, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.99465012550354, 'AUPR-IN': 0.9979754686355591, 'AUPR-OUT': 0.986281156539917, 'FPR95TPR': 0.021400000900030136}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.9581211805343628, 'AUPR-IN': 0.9403345584869385, 'AUPR-OUT': 0.9693204164505005, 'FPR95TPR': 0.2093999981880188}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9959545135498047, 'AUPR-IN': 0.9955841898918152, 'AUPR-OUT': 0.9962907433509827, 'FPR95TPR': 0.01850000023841858}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.981687068939209, 'AUPR-IN': 0.9802305698394775, 'AUPR-OUT': 0.9831022024154663, 'FPR95TPR': 0.08129999786615372}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.9329447150230408, 'AUPR-IN': 0.9786102175712585, 'AUPR-OUT': 0.8011190295219421, 'FPR95TPR': 0.3073999881744385}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9970237612724304, 'AUPR-IN': 0.9929200410842896, 'AUPR-OUT': 0.9980451464653015, 'FPR95TPR': 0.008100000210106373}\n",
      "AUROC: 0.976730227470398\n",
      "AURR: 0.9809425075848898\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR10(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, DMR_single)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.16it/s]\n",
      "[train 002/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.39it/s]\n",
      "[train 003/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.68it/s]\n",
      "[train 004/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.31it/s]\n",
      "[train 005/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.69it/s]\n",
      "[train 006/010]: 100%|##############################################| 390/390 [00:04<00:00, 80.05it/s]\n",
      "[train 007/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.18it/s]\n",
      "[train 008/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.19it/s]\n",
      "[train 009/010]: 100%|##############################################| 390/390 [00:04<00:00, 78.77it/s]\n",
      "[train 010/010]: 100%|##############################################| 390/390 [00:04<00:00, 79.98it/s]\n"
     ]
    }
   ],
   "source": [
    "KIRBY_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_KIRBY\"\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").cuda().eval()\n",
    "KIRBY = train_ood_detector(KIRBY_path, model, batch_size=128, epochs=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.990415632724762, 'AUPR-IN': 0.9963837265968323, 'AUPR-OUT': 0.9748173952102661, 'FPR95TPR': 0.04270000010728836}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.952820897102356, 'AUPR-IN': 0.9385427832603455, 'AUPR-OUT': 0.9624128937721252, 'FPR95TPR': 0.26260000467300415}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9960233569145203, 'AUPR-IN': 0.9957878589630127, 'AUPR-OUT': 0.9963030219078064, 'FPR95TPR': 0.016300000250339508}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9811729788780212, 'AUPR-IN': 0.9803964495658875, 'AUPR-OUT': 0.9820458292961121, 'FPR95TPR': 0.08370000123977661}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.9334304332733154, 'AUPR-IN': 0.9789160490036011, 'AUPR-OUT': 0.804177463054657, 'FPR95TPR': 0.3276999890804291}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.998682975769043, 'AUPR-IN': 0.9974428415298462, 'AUPR-OUT': 0.9991068243980408, 'FPR95TPR': 0.003700000001117587}\n",
      "AUROC: 0.9754243791103363\n",
      "AURR: 0.9812449514865875\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR10(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, KIRBY)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KIRBY + DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 100000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|##############################################| 390/390 [00:06<00:00, 60.60it/s]\n",
      "[train 002/010]: 100%|##############################################| 390/390 [00:06<00:00, 64.70it/s]\n",
      "[train 003/010]: 100%|##############################################| 390/390 [00:05<00:00, 65.89it/s]\n",
      "[train 004/010]: 100%|##############################################| 390/390 [00:05<00:00, 66.13it/s]\n",
      "[train 005/010]: 100%|##############################################| 390/390 [00:05<00:00, 66.35it/s]\n",
      "[train 006/010]: 100%|##############################################| 390/390 [00:05<00:00, 66.18it/s]\n",
      "[train 007/010]: 100%|##############################################| 390/390 [00:05<00:00, 65.74it/s]\n",
      "[train 008/010]: 100%|##############################################| 390/390 [00:05<00:00, 65.73it/s]\n",
      "[train 009/010]: 100%|##############################################| 390/390 [00:05<00:00, 66.15it/s]\n",
      "[train 010/010]: 100%|##############################################| 390/390 [00:05<00:00, 65.60it/s]\n"
     ]
    }
   ],
   "source": [
    "KIRBY_DMR_path = [\"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_KIRBY\",\n",
    "                  \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_single\"]\n",
    "model = WideResNet(num_classes=10, pretrained=\"cifar10-pt\").cuda().eval()\n",
    "KIRBY_DMR = train_ood_detector(KIRBY_DMR_path, model, batch_size=128, epochs=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.9939753413200378, 'AUPR-IN': 0.997723400592804, 'AUPR-OUT': 0.9845340847969055, 'FPR95TPR': 0.024299999698996544}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.9628608226776123, 'AUPR-IN': 0.9482598304748535, 'AUPR-OUT': 0.9719141125679016, 'FPR95TPR': 0.18230000138282776}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9961293339729309, 'AUPR-IN': 0.9958504438400269, 'AUPR-OUT': 0.9964372515678406, 'FPR95TPR': 0.016599999740719795}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9816505908966064, 'AUPR-IN': 0.9805824756622314, 'AUPR-OUT': 0.9828194379806519, 'FPR95TPR': 0.08460000157356262}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.9338406324386597, 'AUPR-IN': 0.9789539575576782, 'AUPR-OUT': 0.8069151043891907, 'FPR95TPR': 0.3089999854564667}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9980031847953796, 'AUPR-IN': 0.995547890663147, 'AUPR-OUT': 0.9986987113952637, 'FPR95TPR': 0.004699999932199717}\n",
      "AUROC: 0.9777433176835378\n",
      "AURR: 0.9828196664651235\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR10(root=\"data\", train=False, download=True, transform=trans)\n",
    "loader_in_train = DataLoader(dataset_in_test, batch_size=128, num_workers=20)\n",
    "\n",
    "detector = Mine(model, KIRBY_DMR)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
