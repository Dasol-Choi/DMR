{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tvt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, SVHN, Places365\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "from pytorch_ood.dataset.img import (\n",
    "    LSUNCrop,\n",
    "    Textures,\n",
    "    TinyImageNetCrop,\n",
    "    GaussianNoise\n",
    ")\n",
    "from pytorch_ood.detector import (\n",
    "    ODIN,\n",
    "    EnergyBased,\n",
    "    Entropy,\n",
    "    KLMatching,\n",
    "    Mahalanobis,\n",
    "    MaxLogit,\n",
    "    MaxSoftmax,\n",
    "    ViM,\n",
    ")\n",
    "from pytorch_ood.model import WideResNet\n",
    "from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "trans = tvt.Compose([\n",
    "    tvt.Resize(size=(32, 32)),\n",
    "    ToRGB(),\n",
    "    tvt.ToTensor(),\n",
    "    tvt.Normalize(std=std, mean=mean)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(test_loader, detector):\n",
    "    metrics = OODMetrics() # Evaluate Detectors\n",
    "    \n",
    "    print(\"Total count:\", len(test_loader))\n",
    "    cnt = 0\n",
    "    for x, y in test_loader:\n",
    "        metrics.update(detector(x.cuda()), y)\n",
    "        cnt += 1\n",
    "        if (cnt + 1) % 50 == 0:\n",
    "            print(f\"[{cnt}/{len(test_loader)}]\")\n",
    "    \n",
    "    r = metrics.compute()\n",
    "    print(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checks.\n",
    "# dataset_out_test = Places365(root=\"data\", split=\"val\", small=True, transform=trans, target_transform=ToUnknown(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Number of GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_all_datasets(detector, base_dataset, transform=trans, batch_size=128, num_workers=20):\n",
    "    dataset_names = [\"SVHN\", \"Textures\", \"LSUNCrop\", \"TinyImageNetCrop\", \"Places365\", \"GaussianNoise\"]\n",
    "    results = []\n",
    "\n",
    "    for name in dataset_names:\n",
    "        if name == \"SVHN\":\n",
    "            dataset_out_test = SVHN(root=\"data\", split=\"test\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"Textures\":\n",
    "            dataset_out_test = Textures(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"LSUNCrop\":\n",
    "            dataset_out_test = LSUNCrop(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"TinyImageNetCrop\":\n",
    "            dataset_out_test = TinyImageNetCrop(root=\"data\", download=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"Places365\":\n",
    "            dataset_out_test = Places365(root=\"data\", split=\"val\", small=True, transform=transform, target_transform=ToUnknown())\n",
    "        elif name == \"GaussianNoise\":\n",
    "            dataset_out_test = GaussianNoise(length=10000, transform=transform, target_transform=ToUnknown())\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        test_loader = DataLoader(torch.utils.data.ConcatDataset([base_dataset, dataset_out_test]), batch_size=batch_size, num_workers=num_workers)\n",
    "        print()\n",
    "        print(name)\n",
    "        results.append(calculate(test_loader, detector))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zZ6VGBU33uhf"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OODDector(nn.Module):\n",
    "    def __init__(self, num_classes=10, multi_class=True):\n",
    "        super().__init__()\n",
    "        self.fc_out = nn.Linear(8192, 2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        if multi_class:\n",
    "            k = num_classes + 1\n",
    "        else:\n",
    "            k = 1\n",
    "        self.out_layer = nn.Linear(2048, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc_out(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hh8wXZG8xI1h"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Any, Tuple\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class CIFAR100withOOD(dset.CIFAR100):\n",
    "    \n",
    "    #torch.manual_seed(42)\n",
    "    def __init__(self, root, batch_size, ood_files=None, download=True, train=True, transform=None,\n",
    "                 ood_transform=None, use_patch_aug=True, multiclass=True):\n",
    "        self.root = root\n",
    "        self.ood_files = ood_files\n",
    "        self.num_classes = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.ood_transform = ood_transform\n",
    "        self.use_patch_aug = use_patch_aug\n",
    "        self.multiclass = multiclass\n",
    "        if multiclass:\n",
    "            self.ood_prob = (self.batch_size / (self.num_classes + 1) * 0.01)\n",
    "        else:\n",
    "            self.ood_prob = 0.5\n",
    "        super().__init__(root, download=download, train=train, transform=transform)\n",
    "\n",
    "    def extract_blocks(self, image, mask):\n",
    "        maximum_score = 8 * 8\n",
    "        patch_size = 8\n",
    "        stride = patch_size // 2\n",
    "        patch_images = []\n",
    "        mask_score = []\n",
    "        for i in range(0, 28, stride):\n",
    "            for j in range(0, 28, stride):\n",
    "                if (j + patch_size) > 32:\n",
    "                    break\n",
    "                patch_image = image[i:i + patch_size, j:j + patch_size]\n",
    "                patch_score = maximum_score - np.sum(mask[i:i + patch_size, j:j + patch_size])\n",
    "                if patch_score == maximum_score:\n",
    "                    if np.random.uniform() >= 0.5:\n",
    "                        resize_image = cv2.resize(patch_image, (32, 32))\n",
    "                    else:\n",
    "                        resize_image = np.tile(patch_image, [4, 4, 1])\n",
    "                    return resize_image\n",
    "                patch_images.append(patch_image)\n",
    "                mask_score.append(patch_score)\n",
    "\n",
    "            if (i + patch_size) > 32:\n",
    "                break\n",
    "\n",
    "        max_idx = np.argmax(mask_score, axis=0)\n",
    "        max_patch = patch_images[max_idx]\n",
    "        if np.random.uniform() >= 0.5:\n",
    "            resize_image = cv2.resize(max_patch, (32, 32))\n",
    "        else:\n",
    "            resize_image = np.tile(max_patch, [4, 4, 1])\n",
    "        return resize_image\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if np.random.uniform() <= self.ood_prob:\n",
    "            rand_idx = np.random.randint(0, len(self.ood_files))\n",
    "            img_file = self.ood_files[rand_idx]\n",
    "            img = np.array(Image.open(img_file).convert('RGB')).astype(np.uint8)\n",
    "            target = self.num_classes\n",
    "\n",
    "            if self.use_patch_aug and np.random.uniform() >= 0.5:\n",
    "                mask_file = img_file.replace(\"_train.png\", \"_mask.png\")\n",
    "                mask = np.array(Image.open(mask_file)) / 255.\n",
    "                mask = np.uint8(mask)\n",
    "                img = self.extract_blocks(img, mask)\n",
    "\n",
    "            img = Image.fromarray(img)\n",
    "            if self.ood_transform is not None:\n",
    "                img = self.ood_transform(img)\n",
    "\n",
    "        else:\n",
    "            img, target = self.data[index], self.targets[index]\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5ZpK58P66m6Z"
   },
   "outputs": [],
   "source": [
    "def get_all_blocks(x, model):\n",
    "    out_list = []\n",
    "    out = model.conv1(x)\n",
    "    out = model.block1(out)\n",
    "    out_list.append(out)\n",
    "    out = model.block2(out)\n",
    "    out_list.append(out)\n",
    "    out = model.block3(out)\n",
    "    out = model.relu(model.bn1(out))\n",
    "    out_list.append(out)\n",
    "    return out_list[2].flatten(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import TypeVar\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Self = TypeVar(\"Self\")\n",
    "\n",
    "\n",
    "class RequiresFittingException(Exception):\n",
    "    \"\"\"\n",
    "    Raised when predict is called on a detector that has not been fitted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg=\"You have to call fit() before predict()\"):\n",
    "        super(RequiresFittingException, self).__init__(msg)\n",
    "\n",
    "\n",
    "class ModelNotSetException(ValueError):\n",
    "    \"\"\"\n",
    "    Raised when predict() is called but no model was given.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg=\"When using predict(), model must not be None\"):\n",
    "        super(ModelNotSetException, self).__init__(msg)\n",
    "\n",
    "\n",
    "class Detector(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Base Class for an Out-of-Distribution Detector\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forwards to predict\n",
    "        \"\"\"\n",
    "        return self.predict(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self: Self, data_loader: DataLoader) -> Self:\n",
    "        \"\"\"\n",
    "        Fit the detector to a dataset. Some methods require this.\n",
    "        :param data_loader: dataset to fit on. This is usually the training dataset.\n",
    "        :raise ModelNotSetException: if model was not set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_features(self: Self, x: Tensor, y: Tensor) -> Self:\n",
    "        \"\"\"\n",
    "        Fit the detector directly on features. Some methods require this.\n",
    "        :param x: training features to use for fitting.\n",
    "        :param y: corresponding class labels.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculates outlier scores. Inputs will be passed through the model.\n",
    "        :param x: batch of data\n",
    "        :return: outlier scores for points\n",
    "        :raise RequiresFitException: if detector has to be fitted to some data\n",
    "        :raise ModelNotSetException: if model was not set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_features(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculates outlier scores based on features.\n",
    "        :param x: batch of data\n",
    "        :return: outlier scores for points\n",
    "        :raise RequiresFitException: if detector has to be fitted to some data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OdOnU77ZE9mE"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, TypeVar\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "Self = TypeVar(\"Self\")\n",
    "\n",
    "\n",
    "class Mine(Detector):\n",
    "    \"\"\"\n",
    "    Implements the Maximum Softmax Probability (MSP) Thresholding baseline for OOD detection.\n",
    "    Optionally, implements temperature scaling, which divides the logits by a constant temperature :math:`T`\n",
    "    before calculating the softmax.\n",
    "    .. math:: - \\\\max_y \\\\sigma_y(f(x) / T)\n",
    "    where :math:`\\\\sigma` is the softmax function and :math:`\\\\sigma_y`  indicates the :math:`y^{th}` value of the\n",
    "    resulting probability vector.\n",
    "    :see Paper:\n",
    "        `ArXiv <https://arxiv.org/abs/1610.02136>`_\n",
    "    :see Implementation:\n",
    "        `GitHub <https://github.com/hendrycks/error-detection>`_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Module, ood_detector: Module, t: Optional[float] = 1.0):\n",
    "        \"\"\"\n",
    "        :param model: neural network to use\n",
    "        :param t: temperature value :math:`T`. Default is 1.\n",
    "        \"\"\"\n",
    "        super(Mine, self).__init__()\n",
    "        self.t = t\n",
    "        self.ood_detector = ood_detector\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param x: input, will be passed through model\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ModelNotSetException\n",
    "\n",
    "        features = get_all_blocks(x, self.model)\n",
    "        pred = self.ood_detector(features.detach())\n",
    "        return pred \n",
    "\n",
    "\n",
    "    def fit(self: Self, *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Not required\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def fit_features(self: Self, *args, **kwargs) -> Self:\n",
    "        \"\"\"\n",
    "        Not required\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict_features(self, logits: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param logits: logits given by the model\n",
    "        \"\"\"\n",
    "        return MaxSoftmax.score(logits, self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as trn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_ood_detector(ood_dataset_path, model, batch_size=128, epochs=10, seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    ood_transform = [\n",
    "        trn.RandomHorizontalFlip(),\n",
    "        trn.RandomAffine(0, (0.0, 0.0), (1.0, 1.5)),\n",
    "        trn.RandomAutocontrast(0.2),\n",
    "        trn.RandomInvert(0.2),\n",
    "        trn.ToTensor(),\n",
    "        trn.Normalize(mean, std),\n",
    "    ]\n",
    "    transform_ood = trn.Compose(ood_transform)\n",
    "    \n",
    "    ind_test_data = dset.CIFAR10(f'data', download=True, train=False, transform=trans)\n",
    "    num_classes = 100\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "    \n",
    "    ind_test_loader = DataLoader(ind_test_data, batch_size=batch_size // 2, shuffle=False, num_workers=8)\n",
    "    \n",
    "    args_multi_class = 0\n",
    "    \n",
    "    if args_multi_class:\n",
    "        multi_class = True\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        multi_class = False\n",
    "        criterion = F.binary_cross_entropy_with_logits\n",
    "    \n",
    "    ood_detector = OODDector(num_classes=num_classes, multi_class=multi_class)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(ood_detector.parameters(), lr=0.01, weight_decay=0.0005, momentum=0.9, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        ood_detector.cuda()\n",
    "    \n",
    "    all_files = []\n",
    "    if isinstance(ood_dataset_path, list):  \n",
    "        for path in ood_dataset_path:\n",
    "            all_files.extend(glob.glob(os.path.join(path, \"*.png\")))\n",
    "    else: \n",
    "        all_files.extend(glob.glob(os.path.join(ood_dataset_path, \"*.png\")))\n",
    "    \n",
    "    print(f\"CAM-based OOD NUM : {len(all_files)}\")\n",
    "    \n",
    "    # ood train dataset\n",
    "    concat_dataset = CIFAR100withOOD(f'data', batch_size, ood_files=all_files,\n",
    "                                    download=True, train=True, transform=trans, ood_transform=transform_ood,\n",
    "                                    use_patch_aug=True, multiclass=multi_class)\n",
    "    train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=20, drop_last=True)\n",
    "    \n",
    "    # train ood classifier\n",
    "    for epoch in range(epochs):\n",
    "        loader = tqdm(train_loader, disable=False, ascii=True)\n",
    "        loader.set_description('[%s %03d/%03d]' % ('train', epoch + 1, epochs))\n",
    "        cnt = 0\n",
    "        for batch_idx, (x_data, y_data) in enumerate(loader):\n",
    "            cnt += batch_size\n",
    "    \n",
    "            if torch.cuda.is_available():\n",
    "                x_data = x_data.cuda()\n",
    "                y_data = y_data.cuda()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            features = get_all_blocks(x_data, model)\n",
    "            # print(features.detach())\n",
    "            pred = ood_detector(features.detach())\n",
    "            # print(pred)\n",
    "            if multi_class:\n",
    "                loss = criterion(pred, y_data.view(-1).cuda().to(torch.long))\n",
    "            else:\n",
    "                y_data = torch.where(y_data == num_classes, torch.ones_like(y_data), torch.zeros_like(y_data))\n",
    "                loss = criterion(pred, y_data.view(-1, 1).cuda().to(torch.float32))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        scheduler.step()\n",
    "\n",
    "    return ood_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.52it/s]\n",
      "[train 002/010]: 100%|#############################################| 390/390 [00:04<00:00, 78.03it/s]\n",
      "[train 003/010]: 100%|#############################################| 390/390 [00:04<00:00, 78.51it/s]\n",
      "[train 004/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.66it/s]\n",
      "[train 005/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.74it/s]\n",
      "[train 006/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.84it/s]\n",
      "[train 007/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.17it/s]\n",
      "[train 008/010]: 100%|#############################################| 390/390 [00:04<00:00, 78.91it/s]\n",
      "[train 009/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.77it/s]\n",
      "[train 010/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.18it/s]\n"
     ]
    }
   ],
   "source": [
    "MLM_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_MLM_without_DMR\"\n",
    "model = WideResNet(num_classes=100, pretrained=\"cifar100-pt\").cuda().eval()\n",
    "MLM = train_ood_detector(MLM_path, model, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.9142085313796997, 'AUPR-IN': 0.967932939529419, 'AUPR-OUT': 0.7631834745407104, 'FPR95TPR': 0.5315999984741211}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.8494260311126709, 'AUPR-IN': 0.8156228065490723, 'AUPR-OUT': 0.8810189366340637, 'FPR95TPR': 0.7257000207901001}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9523019194602966, 'AUPR-IN': 0.9450785517692566, 'AUPR-OUT': 0.9554259777069092, 'FPR95TPR': 0.21570000052452087}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9406701922416687, 'AUPR-IN': 0.9329363107681274, 'AUPR-OUT': 0.9448798298835754, 'FPR95TPR': 0.2517000138759613}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.7627938389778137, 'AUPR-IN': 0.9123202562332153, 'AUPR-OUT': 0.4889182150363922, 'FPR95TPR': 0.7440999746322632}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9989007711410522, 'AUPR-IN': 0.9980148077011108, 'AUPR-OUT': 0.9992934465408325, 'FPR95TPR': 0.002400000113993883}\n",
      "AUROC: 0.9030502140522003\n",
      "AURR: 0.9286509454250336\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR100(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, MLM)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/005]: 100%|#############################################| 390/390 [00:04<00:00, 79.29it/s]\n",
      "[train 002/005]: 100%|#############################################| 390/390 [00:04<00:00, 80.05it/s]\n",
      "[train 003/005]: 100%|#############################################| 390/390 [00:04<00:00, 80.79it/s]\n",
      "[train 004/005]: 100%|#############################################| 390/390 [00:04<00:00, 81.34it/s]\n",
      "[train 005/005]: 100%|#############################################| 390/390 [00:04<00:00, 80.11it/s]\n"
     ]
    }
   ],
   "source": [
    "DMR_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_single\"\n",
    "#DMR_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_multiple\"\n",
    "model = WideResNet(num_classes=100, pretrained=\"cifar100-pt\").cuda().eval()\n",
    "DMR_single = train_ood_detector(DMR_path, model, batch_size=128, epochs=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.986778974533081, 'AUPR-IN': 0.9947327375411987, 'AUPR-OUT': 0.966998815536499, 'FPR95TPR': 0.061000000685453415}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.8731167316436768, 'AUPR-IN': 0.8261831402778625, 'AUPR-OUT': 0.9072492122650146, 'FPR95TPR': 0.5978999733924866}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9710930585861206, 'AUPR-IN': 0.9652902483940125, 'AUPR-OUT': 0.974337637424469, 'FPR95TPR': 0.12890000641345978}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9308744072914124, 'AUPR-IN': 0.911469578742981, 'AUPR-OUT': 0.9391985535621643, 'FPR95TPR': 0.2612999975681305}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.7845720052719116, 'AUPR-IN': 0.9188634753227234, 'AUPR-OUT': 0.5295435786247253, 'FPR95TPR': 0.6956999897956848}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9994666576385498, 'AUPR-IN': 0.9971888661384583, 'AUPR-OUT': 0.9996752738952637, 'FPR95TPR': 0.0010000000474974513}\n",
      "AUROC: 0.9243169724941254\n",
      "AURR: 0.9356213410695394\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR100(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, DMR_single)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 50000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.68it/s]\n",
      "[train 002/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.06it/s]\n",
      "[train 003/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.55it/s]\n",
      "[train 004/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.67it/s]\n",
      "[train 005/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.59it/s]\n",
      "[train 006/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.80it/s]\n",
      "[train 007/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.98it/s]\n",
      "[train 008/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.04it/s]\n",
      "[train 009/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.56it/s]\n",
      "[train 010/010]: 100%|#############################################| 390/390 [00:04<00:00, 81.59it/s]\n"
     ]
    }
   ],
   "source": [
    "DMR_MLM_path = \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_multiple\"\n",
    "model = WideResNet(num_classes=100, pretrained=\"cifar100-pt\").cuda().eval()\n",
    "DMR_MLM = train_ood_detector(DMR_MLM_path, model, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.9785455465316772, 'AUPR-IN': 0.9915254712104797, 'AUPR-OUT': 0.9450326561927795, 'FPR95TPR': 0.10610000044107437}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.8716424107551575, 'AUPR-IN': 0.8239316344261169, 'AUPR-OUT': 0.9052802920341492, 'FPR95TPR': 0.6003999710083008}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.961851179599762, 'AUPR-IN': 0.950957715511322, 'AUPR-OUT': 0.9668123722076416, 'FPR95TPR': 0.1597999930381775}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9316238760948181, 'AUPR-IN': 0.9099531769752502, 'AUPR-OUT': 0.9409030079841614, 'FPR95TPR': 0.25940001010894775}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.7696762681007385, 'AUPR-IN': 0.9147924184799194, 'AUPR-OUT': 0.4788190722465515, 'FPR95TPR': 0.7509999871253967}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9990941286087036, 'AUPR-IN': 0.9964282512664795, 'AUPR-OUT': 0.9994401931762695, 'FPR95TPR': 0.0020000000949949026}\n",
      "AUROC: 0.9187389016151428\n",
      "AURR: 0.9312647779782613\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR100(root=\"data\", train=False, download=True, transform=trans)\n",
    "detector = Mine(model, DMR_MLM)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KIRBY + DMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CAM-based OOD NUM : 100000\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train 001/010]: 100%|#############################################| 390/390 [00:04<00:00, 81.99it/s]\n",
      "[train 002/010]: 100%|#############################################| 390/390 [00:04<00:00, 78.84it/s]\n",
      "[train 003/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.26it/s]\n",
      "[train 004/010]: 100%|#############################################| 390/390 [00:04<00:00, 81.28it/s]\n",
      "[train 005/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.71it/s]\n",
      "[train 006/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.22it/s]\n",
      "[train 007/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.68it/s]\n",
      "[train 008/010]: 100%|#############################################| 390/390 [00:04<00:00, 80.09it/s]\n",
      "[train 009/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.70it/s]\n",
      "[train 010/010]: 100%|#############################################| 390/390 [00:04<00:00, 79.07it/s]\n"
     ]
    }
   ],
   "source": [
    "KIRBY_DMR_path = [\"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_KIRBY\",\n",
    "                  \"/home/dasol/DMR/DMR/CIFAR10_OOD_training_images_using_DMR_single\"]\n",
    "model = WideResNet(num_classes=100, pretrained=\"cifar100-pt\").cuda().eval()\n",
    "KIRBY_DMR = train_ood_detector(KIRBY_DMR_path, model, batch_size=128, epochs=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "\n",
      "SVHN\n",
      "Total count: 282\n",
      "[49/282]\n",
      "[99/282]\n",
      "[149/282]\n",
      "[199/282]\n",
      "[249/282]\n",
      "{'AUROC': 0.9839572310447693, 'AUPR-IN': 0.993377685546875, 'AUPR-OUT': 0.9614638686180115, 'FPR95TPR': 0.07660000026226044}\n",
      "\n",
      "Textures\n",
      "Total count: 123\n",
      "[49/123]\n",
      "[99/123]\n",
      "{'AUROC': 0.9165979623794556, 'AUPR-IN': 0.8775882124900818, 'AUPR-OUT': 0.9427220821380615, 'FPR95TPR': 0.39730000495910645}\n",
      "\n",
      "LSUNCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9690408110618591, 'AUPR-IN': 0.9615352153778076, 'AUPR-OUT': 0.9728801846504211, 'FPR95TPR': 0.13289999961853027}\n",
      "\n",
      "TinyImageNetCrop\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9272008538246155, 'AUPR-IN': 0.9092554450035095, 'AUPR-OUT': 0.933953583240509, 'FPR95TPR': 0.28439998626708984}\n",
      "\n",
      "Places365\n",
      "Total count: 364\n",
      "[49/364]\n",
      "[99/364]\n",
      "[149/364]\n",
      "[199/364]\n",
      "[249/364]\n",
      "[299/364]\n",
      "[349/364]\n",
      "{'AUROC': 0.7871794700622559, 'AUPR-IN': 0.9210013151168823, 'AUPR-OUT': 0.5281116366386414, 'FPR95TPR': 0.6973000168800354}\n",
      "\n",
      "GaussianNoise\n",
      "Total count: 157\n",
      "[49/157]\n",
      "[99/157]\n",
      "[149/157]\n",
      "{'AUROC': 0.9995114207267761, 'AUPR-IN': 0.9974625110626221, 'AUPR-OUT': 0.9997018575668335, 'FPR95TPR': 0.0010999999940395355}\n",
      "AUROC: 0.930581291516622\n",
      "AURR: 0.9433700640996298\n"
     ]
    }
   ],
   "source": [
    "dataset_in_test = CIFAR100(root=\"data\", train=False, download=True, transform=trans)\n",
    "loader_in_train = DataLoader(dataset_in_test, batch_size=128, num_workers=20)\n",
    "\n",
    "detector = Mine(model, KIRBY_DMR)\n",
    "results = test_with_all_datasets(detector, dataset_in_test)\n",
    "\n",
    "mean_AUROC = np.mean([r[\"AUROC\"] for r in results])\n",
    "mean_AUPR = np.mean([r['AUPR-IN'] for r in results])\n",
    "\n",
    "print(\"AUROC:\", mean_AUROC)\n",
    "print(\"AURR:\", mean_AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
